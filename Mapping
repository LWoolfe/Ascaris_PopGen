# Analysis of population structure of Ascaris sp.  

## Table of contents
1. Project setup
2. Raw data
3. Mapping


## 01 - Project setup 
# Setup a working environment for the analysis.
mkdir Pop_gen
cd Pop_gen

# make working directories
mkdir 00_METADATA 01-reference 02-raw_sequences 03-mapping 04-variants 05-QC 06-analysis
```
## 02 - Raw Data
# Download publicly available WGS reads using SRA toolkit
# Trim and perform fastqc on all data 

fastp 
fast qc 
multiqc

# Download the A. suum reference genome on ncbi using SRA Toolkit 
cd 01-reference
prefetch GCA_013433145.1

# Validate the downloaded genome 
vdb-validate GCA_013433145.1

# Unzip
gunzip GCA_013433145.1_ASM1343314V1_genomic.fa.gz

#Ensure that GCA_013433145.1_ASM1343314v1_assembly_report.txt was downloaded, then create a list of primary scaffolds
#Check headers are correct
grep -v "^#" GCA_013433145.1_ASM1343314v1_assembly_report.txt | head
grep -v "^#" GCA_013433145.1_ASM1343314v1_assembly_report.txt \
  | awk -F '\t' '$2 == "assembled-molecule" && ($1 ~ /^chr[0-9]+$/ || $1 == "chrX") {print $5}' \
  > scaffolds_primary_plus_X.txt

# Exclude unmapped scaffolds (include only primary scaffolds)
seqtk subseq genome.fna scaffolds_primary_plus_X.txt > Ascaris_primary_scaffolds_plus_X.fasta

# Create indexes and a sequence dictionary for the reference genome
bwa index Ascaris_primary_scaffolds_plus_X.fasta
samtools faidx Ascaris_primary_scaffolds_plus_X.fasta
gatk CreateSequenceDictionary --REFERENCE Ascaris_primary_scaffolds_plus_X.fasta

# Check all primary scaffolds are present
grep -v "^#" GCA_013433145.1_ASM1343314v1_assembly_report.txt \
  | awk -F '\t' '$2 == "assembled-molecule" && $1 ~ /^chr[0-9]+$/ {print $1}' \
  | wc -l
wc -l scaffolds_primary.txt

## 03 - Mapping 
# Map sequence reads to reference genome

cd 03-mapping
conda activate BWA
chmod +x mapping.sh
./mapping.sh               #see script below

#!/usr/bin/env bash
set -euo pipefail
shopt -s nullglob

# --- Autoâ€‘detect system resources ---
THREADS_TOTAL=$(nproc)  # total CPU cores
TOTAL_MEM_GB=$(awk '/MemTotal/ {printf "%.0f", $2/1024/1024}' /proc/meminfo)  

# --- Resource allocation policy ---
RESERVE_PCT=20                  # % RAM to leave for OS/background
JOBS=4                           # samples processed in parallel
THREADS_PER_JOB=$(( THREADS_TOTAL / JOBS ))
(( THREADS_PER_JOB >= 1 )) || THREADS_PER_JOB=1

PER_JOB_MEM="$(( (TOTAL_MEM_GB * (100 - RESERVE_PCT) / 100) / JOBS ))G"

# --- Hard cap per-job memory to avoid mlock/ulimit issues ---
MAX_SORT_MEM="4G"
if (( ${PER_JOB_MEM%G} > ${MAX_SORT_MEM%G} )); then
    SAMTOOLS_SORT_MEM="${MAX_SORT_MEM}"
else
    SAMTOOLS_SORT_MEM="${PER_JOB_MEM}"
fi

# Prefer fast local scratch if available
: "${TMPDIR:=/tmp}"

REF_DIR="01-reference"
READS_DIR="02-raw_reads"
OUT_DIR="03-mapping"
QC_DIR="${OUT_DIR}/qc"
REF_GENOME="${REF_DIR}/Ascaris_primary_scaffolds_plus_X.fasta"

mkdir -p "${OUT_DIR}" "${QC_DIR}"

# --- Preflight checks ---
R1_FILES=( "${READS_DIR}"/*_trimmed_R1.fq.gz )
if (( ${#R1_FILES[@]} == 0 )); then
  echo "ERROR: no R1 FASTQs found in ${READS_DIR} matching *_trimmed_R1.fq.gz" >&2
  exit 1
fi

[[ -s "${REF_GENOME}" ]] || { echo "ERROR: missing reference FASTA ${REF_GENOME}" >&2; exit 1; }

if ! ls "${REF_GENOME}".* 1>/dev/null 2>&1; then
  echo "Indexing reference for bwa..." >&2
  bwa index "${REF_GENOME}"
fi

[[ -s "${REF_GENOME}.fai" ]] || samtools faidx "${REF_GENOME}"

process_sample() {
  local R1="$1"
  local SAMPLE R2 HEADER FLOWCELL LANE PU RGID BAM

  SAMPLE="$(basename "$R1" _trimmed_R1.fq.gz)"
  R2="${READS_DIR}/${SAMPLE}_trimmed_R2.fq.gz"
  BAM="${OUT_DIR}/${SAMPLE}.bam"

  [[ -s "$R2" ]] || { echo "ERROR: missing R2 for ${SAMPLE}" >&2; return 1; }

  echo "$(date +'%F %T') $$ Mapping ${SAMPLE} (threads=${THREADS_PER_JOB}, mem=${SAMTOOLS_SORT_MEM})" >&2

  if ! read -r HEADER < <(zcat -f -- "$R1" 2>/dev/null | head -n1); then
    HEADER=""
  fi

  FLOWCELL="$(awk -F':' '{if(NF>=3) print $3}' <<<"$HEADER")"
  LANE="$(awk -F':' '{if(NF>=4) print $4}' <<<"$HEADER")"
  if [[ -n "${FLOWCELL}" && -n "${LANE}" ]]; then
    PU="${FLOWCELL}.${LANE}"; RGID="${PU}"
  else
    PU="${SAMPLE}.PU"; RGID="${SAMPLE}"
  fi

  # Map + sort to BAM
  bwa mem -t "${THREADS_PER_JOB}" \
    -R "@RG\tID:${RGID}\tSM:${SAMPLE}\tPL:ILLUMINA\tLB:lib1\tPU:${PU}" \
    "${REF_GENOME}" "${R1}" "${R2}" \
  | samtools sort -@"${THREADS_PER_JOB}" -m "${SAMTOOLS_SORT_MEM}" -T "${TMPDIR}/${SAMPLE}.sort" \
      -o "${BAM}" - \
  || { echo "ERROR: sort failed for ${SAMPLE}" >&2; return 1; }

  # Index + QC
  samtools index -@"${THREADS_PER_JOB}" "${BAM}"

  samtools view -H "${BAM}" | grep -q '^@RG' \
    || { echo "ERROR: @RG missing in ${SAMPLE}.bam" >&2; return 1; }

  samtools flagstat "${BAM}" > "${QC_DIR}/${SAMPLE}.flagstat.txt"
  [[ -s "${QC_DIR}/${SAMPLE}.flagstat.txt" ]] \
    || { echo "ERROR: flagstat failed for ${SAMPLE}" >&2; return 1; }

  echo "$(date +'%F %T') $$ Finished ${SAMPLE}" >&2
}

export -f process_sample
export READS_DIR OUT_DIR QC_DIR REF_GENOME THREADS_PER_JOB SAMTOOLS_SORT_MEM TMPDIR

# Prefer GNU parallel
if command -v parallel >/dev/null 2>&1; then
  parallel --jobs "${JOBS}" --eta --joblog "${QC_DIR}/parallel.log" \
    --halt soon,fail=1 \
    --env process_sample --env READS_DIR --env OUT_DIR --env QC_DIR --env REF_GENOME \
    --env THREADS_PER_JOB --env SAMTOOLS_SORT_MEM --env TMPDIR \
    bash -c 'process_sample "$@"' _ ::: "${R1_FILES[@]}"
else
  printf '%s\0' "${R1_FILES[@]}" | xargs -0 -n1 -P "${JOBS}" bash -c 'process_sample "$0"'
fi

conda deactivate 

## Mark PCR duplicates
conda activate GATK
chmod +x Markdup.sh
./Markdup.sh    #see script below

#!/bin/bash
set -e

MAX_JOBS=4
QC_DIR="qc_metrics"
mkdir -p "$QC_DIR"

for bamfile in *.bam; do
  sample="${bamfile%.bam}"
  echo "ðŸš€ Starting $sample..."

  (
    # Mark duplicates
    gatk MarkDuplicates \
      --INPUT "$bamfile" \
      --OUTPUT "${sample}.markdup.bam" \
      --METRICS_FILE "${sample}.metrics.txt"

    # Index the marked BAM
    samtools index "${sample}.markdup.bam"

    # Run flagstat on marked BAM
    samtools flagstat "${sample}.markdup.bam" > "${QC_DIR}/${sample}.flagstat.txt"

    # Check flagstat output
    if [[ -s "${QC_DIR}/${sample}.flagstat.txt" ]]; then
      echo "$(date +'%F %T') $$ âœ… Finished ${sample}" >&2
    else
      echo "âŒ ERROR: flagstat failed for ${sample}" >&2
      exit 1
    fi
  ) &

  # Limit number of concurrent jobs
  while (( $(jobs -rp | wc -l) >= MAX_JOBS )); do
    echo "â³ Currently running: $(jobs -rp | wc -l) jobs..."
    sleep 1
  done
done

wait
echo "ðŸŽ‰ All samples processed and QC complete."

## Assessment of Mapping and Preprocessing. 
#use samtools to depth per base to create 10kb windows for duther analysis. 

./coverage_pipline.sh #script below.

#!/bin/bash
set -e

W=10000  # window size in bp

export W

# ---------------------------
# Step 1: Process BAMs in parallel
# ---------------------------

process_bam() {
    bam="$1"
    sample=${bam%.markdup.bam}
    echo "Processing $sample..."

    samtools depth -aa "$bam" \
    | awk -v W="$W" '
      BEGIN { OFS="\t"; prev_chr=""; sum=0; cnt=0; cov=0 }
      {
        chr=$1; pos=$2; d=$3
        bin=int((pos-1)/W)
        if (chr != prev_chr || bin != cur_bin) {
          if (cnt) {
            start=cur_bin*W; end=start+W
            mean=sum/cnt; frac=cov/cnt
            print prev_chr, start, end, mean, frac
          }
          prev_chr=chr; cur_bin=bin
          sum=d; cnt=1; cov=(d>0?1:0)
        } else {
          sum+=d; cnt++; cov+=(d>0?1:0)
        }
      }
      END {
        if (cnt) {
          start=cur_bin*W; end=start+W
          mean=sum/cnt; frac=cov/cnt
          print prev_chr, start, end, mean, frac
        }
      }' > "${sample}.metrics10kb.bed"
}

export -f process_bam

# Run in parallel across all BAMs
parallel process_bam ::: *.markdup.bam

# ---------------------------
# Step 2: Merge into matrices
# ---------------------------

# Get window coordinates from the first file
first_file=$(ls *.metrics10kb.bed | head -n 1)
cut -f1-3 "$first_file" > windows.tmp

# Build header
header="chr\tstart\tend"
for f in *.metrics10kb.bed; do
  sample=${f%.metrics10kb.bed}
  header+="\t$sample"
done

# Write headers
echo -e "$header" > depth_matrix.tsv
echo -e "$header" > coverage_matrix.tsv

# Initialise bodies
cp windows.tmp depth_body.tmp
cp windows.tmp coverage_body.tmp

# Append each sampleâ€™s columns
for f in *.metrics10kb.bed; do
  cut -f4 "$f" > depth.tmp
  cut -f5 "$f" > coverage.tmp
  paste depth_body.tmp depth.tmp > tmp && mv tmp depth_body.tmp
  paste coverage_body.tmp coverage.tmp > tmp && mv tmp coverage_body.tmp
  rm depth.tmp coverage.tmp
done

# Combine headers + bodies
cat depth_matrix.tsv depth_body.tmp > mean_depth_matrix.tsv
cat coverage_matrix.tsv coverage_body.tmp > coverage_fraction_matrix.tsv

# Clean up temp files
rm windows.tmp depth_body.tmp coverage_body.tmp depth_matrix.tsv coverage_matrix.tsv

# ---------------------------
# Step 3: Median & stdev across samples
# ---------------------------

# Function to summarise a matrix
summarise_matrix() {
    matrix="$1"
    out="$2"
    # Get number of columns
    last_col=$(awk '{print NF}' "$matrix" | sort -nu | tail -n1)
    datamash -H -t $'\t' \
        groupby 1,2,3 \
        median 4-"$last_col" \
        sstdev 4-"$last_col" \
        < "$matrix" > "$out"
}

summarise_matrix coverage_fraction_matrix.tsv coverage_fraction_summary.tsv
summarise_matrix mean_depth_matrix.tsv mean_depth_summary.tsv

echo "âœ… All done!"
echo "Outputs:"
echo " - mean_depth_matrix.tsv"
echo " - coverage_fraction_matrix.tsv"
echo " - mean_depth_summary.tsv"
echo " - coverage_fraction_summary.tsv"










