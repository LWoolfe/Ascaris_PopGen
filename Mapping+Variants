# Analysis of population structure of Ascaris sp.  

## Table of contents
1. Project setup
2. Raw data
3. Mapping


## 01 - Project setup 
# Setup a working environment for the analysis.
mkdir Pop_gen
cd Pop_gen

# make working directories
mkdir 00_METADATA 01-reference 02-raw_sequences 03-mapping 04-variants 05-QC 06-analysis
```
## 02 - Raw Data
# Download publicly available WGS reads using SRA toolkit
# Trim and perform fastqc on all data 

fastp 
fast qc 
multiqc

# Download the A. suum reference genome on ncbi using SRA Toolkit 
cd 01-reference
prefetch GCA_013433145.1

# Validate the downloaded genome 
vdb-validate GCA_013433145.1

# Unzip
gunzip GCA_013433145.1_ASM1343314V1_genomic.fa.gz

#Ensure that GCA_013433145.1_ASM1343314v1_assembly_report.txt was downloaded, then create a list of primary scaffolds
#Check headers are correct
grep -v "^#" GCA_013433145.1_ASM1343314v1_assembly_report.txt | head
grep -v "^#" GCA_013433145.1_ASM1343314v1_assembly_report.txt \
  | awk -F '\t' '$2 == "assembled-molecule" && ($1 ~ /^chr[0-9]+$/ || $1 == "chrX") {print $5}' \
  > scaffolds_primary_plus_X.txt

# Exclude unmapped scaffolds (include only primary scaffolds)
seqtk subseq genome.fna scaffolds_primary_plus_X.txt > Ascaris_primary_scaffolds_plus_X.fasta

# Create indexes and a sequence dictionary for the reference genome
bwa index Ascaris_primary_scaffolds_plus_X.fasta
samtools faidx Ascaris_primary_scaffolds_plus_X.fasta
gatk CreateSequenceDictionary --REFERENCE Ascaris_primary_scaffolds_plus_X.fasta

# Check all primary scaffolds are present
grep -v "^#" GCA_013433145.1_ASM1343314v1_assembly_report.txt \
  | awk -F '\t' '$2 == "assembled-molecule" && $1 ~ /^chr[0-9]+$/ {print $1}' \
  | wc -l
wc -l scaffolds_primary.txt

conda activate gatk 
gatk CreateSequenceDictionary -R Ascaris_primary_scaffolds_plus_X.fa

## 03 - Mapping 
# Map sequence reads to reference genome

cd 03-mapping
conda activate BWA
chmod +x mapping.sh
./mapping.sh               #see script below

#!/usr/bin/env bash
set -euo pipefail
shopt -s nullglob

# --- Autoâ€‘detect system resources ---
THREADS_TOTAL=$(nproc)  # total CPU cores
TOTAL_MEM_GB=$(awk '/MemTotal/ {printf "%.0f", $2/1024/1024}' /proc/meminfo)  

# --- Resource allocation policy ---
RESERVE_PCT=20                  # % RAM to leave for OS/background
JOBS=4                           # samples processed in parallel
THREADS_PER_JOB=$(( THREADS_TOTAL / JOBS ))
(( THREADS_PER_JOB >= 1 )) || THREADS_PER_JOB=1

PER_JOB_MEM="$(( (TOTAL_MEM_GB * (100 - RESERVE_PCT) / 100) / JOBS ))G"

# --- Hard cap per-job memory to avoid mlock/ulimit issues ---
MAX_SORT_MEM="4G"
if (( ${PER_JOB_MEM%G} > ${MAX_SORT_MEM%G} )); then
    SAMTOOLS_SORT_MEM="${MAX_SORT_MEM}"
else
    SAMTOOLS_SORT_MEM="${PER_JOB_MEM}"
fi

# Prefer fast local scratch if available
: "${TMPDIR:=/tmp}"

REF_DIR="01-reference"
READS_DIR="02-raw_reads"
OUT_DIR="03-mapping"
QC_DIR="${OUT_DIR}/qc"
REF_GENOME="${REF_DIR}/Ascaris_primary_scaffolds_plus_X.fasta"

mkdir -p "${OUT_DIR}" "${QC_DIR}"

# --- Preflight checks ---
R1_FILES=( "${READS_DIR}"/*_trimmed_R1.fq.gz )
if (( ${#R1_FILES[@]} == 0 )); then
  echo "ERROR: no R1 FASTQs found in ${READS_DIR} matching *_trimmed_R1.fq.gz" >&2
  exit 1
fi

[[ -s "${REF_GENOME}" ]] || { echo "ERROR: missing reference FASTA ${REF_GENOME}" >&2; exit 1; }

if ! ls "${REF_GENOME}".* 1>/dev/null 2>&1; then
  echo "Indexing reference for bwa..." >&2
  bwa index "${REF_GENOME}"
fi

[[ -s "${REF_GENOME}.fai" ]] || samtools faidx "${REF_GENOME}"

process_sample() {
  local R1="$1"
  local SAMPLE R2 HEADER FLOWCELL LANE PU RGID BAM

  SAMPLE="$(basename "$R1" _trimmed_R1.fq.gz)"
  R2="${READS_DIR}/${SAMPLE}_trimmed_R2.fq.gz"
  BAM="${OUT_DIR}/${SAMPLE}.bam"

  [[ -s "$R2" ]] || { echo "ERROR: missing R2 for ${SAMPLE}" >&2; return 1; }

  echo "$(date +'%F %T') $$ Mapping ${SAMPLE} (threads=${THREADS_PER_JOB}, mem=${SAMTOOLS_SORT_MEM})" >&2

  if ! read -r HEADER < <(zcat -f -- "$R1" 2>/dev/null | head -n1); then
    HEADER=""
  fi

  FLOWCELL="$(awk -F':' '{if(NF>=3) print $3}' <<<"$HEADER")"
  LANE="$(awk -F':' '{if(NF>=4) print $4}' <<<"$HEADER")"
  if [[ -n "${FLOWCELL}" && -n "${LANE}" ]]; then
    PU="${FLOWCELL}.${LANE}"; RGID="${PU}"
  else
    PU="${SAMPLE}.PU"; RGID="${SAMPLE}"
  fi

  # Map + sort to BAM
  bwa mem -t "${THREADS_PER_JOB}" \
    -R "@RG\tID:${RGID}\tSM:${SAMPLE}\tPL:ILLUMINA\tLB:lib1\tPU:${PU}" \
    "${REF_GENOME}" "${R1}" "${R2}" \
  | samtools sort -@"${THREADS_PER_JOB}" -m "${SAMTOOLS_SORT_MEM}" -T "${TMPDIR}/${SAMPLE}.sort" \
      -o "${BAM}" - \
  || { echo "ERROR: sort failed for ${SAMPLE}" >&2; return 1; }

  # Index + QC
  samtools index -@"${THREADS_PER_JOB}" "${BAM}"

  samtools view -H "${BAM}" | grep -q '^@RG' \
    || { echo "ERROR: @RG missing in ${SAMPLE}.bam" >&2; return 1; }

  samtools flagstat "${BAM}" > "${QC_DIR}/${SAMPLE}.flagstat.txt"
  [[ -s "${QC_DIR}/${SAMPLE}.flagstat.txt" ]] \
    || { echo "ERROR: flagstat failed for ${SAMPLE}" >&2; return 1; }

  echo "$(date +'%F %T') $$ Finished ${SAMPLE}" >&2
}

export -f process_sample
export READS_DIR OUT_DIR QC_DIR REF_GENOME THREADS_PER_JOB SAMTOOLS_SORT_MEM TMPDIR

# Prefer GNU parallel
if command -v parallel >/dev/null 2>&1; then
  parallel --jobs "${JOBS}" --eta --joblog "${QC_DIR}/parallel.log" \
    --halt soon,fail=1 \
    --env process_sample --env READS_DIR --env OUT_DIR --env QC_DIR --env REF_GENOME \
    --env THREADS_PER_JOB --env SAMTOOLS_SORT_MEM --env TMPDIR \
    bash -c 'process_sample "$@"' _ ::: "${R1_FILES[@]}"
else
  printf '%s\0' "${R1_FILES[@]}" | xargs -0 -n1 -P "${JOBS}" bash -c 'process_sample "$0"'
fi

conda deactivate 

## Mark PCR duplicates
conda activate GATK
chmod +x Markdup.sh
./Markdup.sh    #see script below

#!/bin/bash
set -e

MAX_JOBS=4
QC_DIR="qc_metrics"
mkdir -p "$QC_DIR"

for bamfile in *.bam; do
  sample="${bamfile%.bam}"
  echo "ðŸš€ Starting $sample..."

  (
    # Mark duplicates
    gatk MarkDuplicates \
      --INPUT "$bamfile" \
      --OUTPUT "${sample}.markdup.bam" \
      --METRICS_FILE "${sample}.metrics.txt"

    # Index the marked BAM
    samtools index "${sample}.markdup.bam"

    # Run flagstat on marked BAM
    samtools flagstat "${sample}.markdup.bam" > "${QC_DIR}/${sample}.flagstat.txt"

    # Check flagstat output
    if [[ -s "${QC_DIR}/${sample}.flagstat.txt" ]]; then
      echo "$(date +'%F %T') $$ âœ… Finished ${sample}" >&2
    else
      echo "âŒ ERROR: flagstat failed for ${sample}" >&2
      exit 1
    fi
  ) &

  # Limit number of concurrent jobs
  while (( $(jobs -rp | wc -l) >= MAX_JOBS )); do
    echo "â³ Currently running: $(jobs -rp | wc -l) jobs..."
    sleep 1
  done
done

wait
echo "ðŸŽ‰ All samples processed and QC complete."

## Assessment of Mapping and Preprocessing. 
#use samtools to depth per base to create 10kb windows for duther analysis. 

./coverage_pipline.sh #script below.
#!/bin/bash
set -e

W=10000  # window size in bp

export W

# ---------------------------
# Step 1: Process BAMs in parallel (or sequential if parallel not installed)
# ---------------------------

process_bam() {
    bam="$1"
    sample=${bam%.markdup.bam}
    echo "Processing $sample..."

    samtools depth -aa "$bam" \
    | awk -v W="$W" '
      BEGIN { OFS="\t"; prev_chr=""; sum=0; cnt=0; cov=0 }
      {
        chr=$1; pos=$2; d=$3
        bin=int((pos-1)/W)
        if (chr != prev_chr || bin != cur_bin) {
          if (cnt) {
            start=cur_bin*W; end=start+W
            mean=sum/cnt; frac=cov/cnt
            print prev_chr, start, end, mean, frac
          }
          prev_chr=chr; cur_bin=bin
          sum=d; cnt=1; cov=(d>0?1:0)
        } else {
          sum+=d; cnt++; cov+=(d>0?1:0)
        }
      }
      END {
        if (cnt) {
          start=cur_bin*W; end=start+W
          mean=sum/cnt; frac=cov/cnt
          print prev_chr, start, end, mean, frac
        }
      }' > "${sample}.metrics10kb.bed"
}

export -f process_bam

if command -v parallel >/dev/null 2>&1; then
    parallel process_bam ::: *.markdup.bam
else
    echo "âš ï¸ GNU parallel not found â€” running sequentially"
    for bam in *.markdup.bam; do
        process_bam "$bam"
    done
fi

# ---------------------------
# Step 2: Merge into matrices
# ---------------------------

first_file=$(ls *.metrics10kb.bed | head -n 1)
cut -f1-3 "$first_file" > windows.tmp

header="chr\tstart\tend"
for f in *.metrics10kb.bed; do
  sample=${f%.metrics10kb.bed}
  header+="\t$sample"
done

echo -e "$header" > depth_matrix.tsv
echo -e "$header" > coverage_matrix.tsv

cp windows.tmp depth_body.tmp
cp windows.tmp coverage_body.tmp

for f in *.metrics10kb.bed; do
  cut -f4 "$f" > depth.tmp
  cut -f5 "$f" > coverage.tmp
  paste depth_body.tmp depth.tmp > tmp && mv tmp depth_body.tmp
  paste coverage_body.tmp coverage.tmp > tmp && mv tmp coverage_body.tmp
  rm depth.tmp coverage.tmp
done

cat depth_matrix.tsv depth_body.tmp > mean_depth_matrix.tsv
cat coverage_matrix.tsv coverage_body.tmp > coverage_fraction_matrix.tsv

rm windows.tmp depth_body.tmp coverage_body.tmp depth_matrix.tsv coverage_matrix.tsv

# ---------------------------
# Step 3: Median & stdev across samples (reshape to long format first)
# ---------------------------

summarise_matrix() {
    matrix="$1"
    out="$2"
    tail -n +2 "$matrix" \
    | awk 'BEGIN{OFS="\t"} {chr=$1; start=$2; end=$3; for(i=4;i<=NF;i++){print chr, start, end, $i}}' \
    | datamash -g 1,2,3 median 4 sstdev 4 \
    > "$out"
}

summarise_matrix coverage_fraction_matrix.tsv coverage_fraction_summary.tsv
summarise_matrix mean_depth_matrix.tsv mean_depth_summary.tsv

echo "âœ… All done!"
echo "Outputs:"
echo " - mean_depth_matrix.tsv"
echo " - coverage_fraction_matrix.tsv"
echo " - mean_depth_summary.tsv"
echo " - coverage_fraction_summary.tsv"

###Variant Calling 
./run_haplotypecaller.sh #script below

#!/bin/bash
set -euo pipefail

# Define directories
MAP_DIR="03-mapping"
REF_DIR="01-reference"
VAR_DIR="04-variants"

# Make sure output directory exists
mkdir -p "$VAR_DIR"

# Function to run HaplotypeCaller for one BAM
call_variants() {
    bam="$1"
    sample=$(basename "$bam" .bam)
    echo "Calling variants for $sample..."
    gatk HaplotypeCaller \
        --emit-ref-confidence GVCF \
        -I "$bam" \
        -R "$REF_DIR/Ascaris_primary_scaffolds_plus_X.fasta" \
        -O "$VAR_DIR/${sample}.g.vcf.gz"
    tabix -p vcf "$VAR_DIR/${sample}.g.vcf.gz"
}

export -f call_variants
export REF_DIR VAR_DIR

# Run in parallel if available, else sequentially
if command -v parallel >/dev/null 2>&1; then
    parallel call_variants ::: "$MAP_DIR"/*.bam
else
    echo "âš ï¸ GNU parallel not found â€” running sequentially"
    for bam in "$MAP_DIR"/*.bam; do
        call_variants "$bam"
    done
fi

echo "âœ… All GVCFs are in $VAR_DIR"




Conda activate gatk 
./genotyping.sh  # see script below

#!/bin/bash
set -euo pipefail

# -----------------------------
# CONFIGURATION
# -----------------------------
VAR_DIR="04-variants"   # folder with per-sample gVCFs
REF_DIR="01-reference"  # folder with reference FASTA
REF_FASTA="Ascaris_primary_scaffolds_plus_X.fasta"

# Output file names for preliminary run
ARG_LIST="argument_prelim.list"
COMBINED_GVCF="merged_prelim_samples.g.vcf.gz"
FINAL_VCF="merged_prelim_samples.vcf"
QC_TABLE="cohort_prelim.genotyped.txt"

# -----------------------------
# STEP 1: Build argument list
# -----------------------------
echo "ðŸ“„ Building $ARG_LIST from gVCFs in $VAR_DIR..."
> "$ARG_LIST"
for gvcf in "$VAR_DIR"/*.g.vcf.gz; do
    echo "--variant $gvcf" >> "$ARG_LIST"
done
echo "âœ… Created $ARG_LIST:"
cat "$ARG_LIST"

# -----------------------------
# STEP 2: Combine GVCFs
# -----------------------------
echo "ðŸ”„ Combining GVCFs..."
gatk CombineGVCFs \
    --arguments_file "$ARG_LIST" \
    --reference "$REF_DIR/$REF_FASTA" \
    --output "$COMBINED_GVCF"

# -----------------------------
# STEP 3: Joint Genotyping
# -----------------------------
echo "ðŸ§¬ Running joint genotyping..."
gatk GenotypeGVCFs \
    --reference "$REF_DIR/$REF_FASTA" \
    --variant "$COMBINED_GVCF" \
    --output "$FINAL_VCF"\
     --native-pair-hmm-threads 16

# -----------------------------
# STEP 4: QC Table
# -----------------------------
echo "ðŸ“Š Extracting QC metrics..."
gatk VariantsToTable \
    --variant "$FINAL_VCF" \
    -F CHROM -F POS -F TYPE -F QD -F FS -F MQ -F MQRankSum \
    -F ReadPosRankSum -F SOR -F InbreedingCoeff \
    -R "$REF_DIR/$REF_FASTA" \
    --output "$QC_TABLE"

echo "ðŸŽ‰ Preliminary run complete!"
echo "Outputs:"
echo " - Combined GVCF: $COMBINED_GVCF"
echo " - Final multi-sample VCF: $FINAL_VCF"
echo " - QC metrics table: $QC_TABLE"










